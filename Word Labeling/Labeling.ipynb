{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-HW2.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5spMTBJPKs7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RpOv2Nejybf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9nYon_Yjyzv"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/NLP-HW2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcel_OMljsi2"
      },
      "source": [
        "# Importing libraries\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import pprint, time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkWfffcdjsi8"
      },
      "source": [
        "path = data_dir + '/Train.txt'\n",
        "file = open(path, 'r', encoding=\"utf-8\")\n",
        "content = file.read()\n",
        "content = content.split('\\n\\n')\n",
        "train_data = [list(map(lambda x:(x.split(' ')[0],x.split(' ')[1]),c.split('\\n'))) for c in content]\n",
        "train_data[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nr4fwNjjsi-"
      },
      "source": [
        "path = data_dir + '/Test.txt'\n",
        "file = open(path, 'r', encoding=\"utf-8\")\n",
        "content = file.read()\n",
        "content = content.split('\\n\\n')\n",
        "test_data = []\n",
        "for c in content:\n",
        "    items = c.split('\\n')\n",
        "    new_items = []\n",
        "    for it in items:\n",
        "        splitted = it.split(' ')\n",
        "        if len(splitted) == 2:\n",
        "            new_items.append((splitted[0],splitted[1]))\n",
        "    test_data.append(new_items)\n",
        "test_data[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRE5QGfLjsi-"
      },
      "source": [
        "train_tagged_words = [ tup for sent in train_data for tup in sent ]\n",
        "test_tagged_words = [ tup for sent in test_data for tup in sent ]\n",
        "print(len(train_tagged_words))\n",
        "print(len(test_tagged_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbvCZ_T_jsi_"
      },
      "source": [
        "# check some of the tagged words.\n",
        "print(train_tagged_words[:5])\n",
        "print(test_tagged_words[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SiwWdCfjsjA"
      },
      "source": [
        "#use set datatype to check how many unique tags are present in training data\n",
        "tags = {tag for word,tag in train_tagged_words}\n",
        "print(len(tags))\n",
        "print(tags)\n",
        " \n",
        "# check total words in vocabulary\n",
        "vocab = {word for word,tag in train_tagged_words}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn1_7mXajsjA"
      },
      "source": [
        "len(vocab), vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrufEKrZjsjB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gwr0ViGjsjC"
      },
      "source": [
        "# compute Emission Probability\n",
        "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
        "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
        "    count_tag = len(tag_list)#total number of times the passed tag occurred in train_bag\n",
        "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
        "#now calculate the total number of times the passed word occurred as the passed tag.\n",
        "    count_w_given_tag = len(w_given_tag_list)\n",
        "    #print(count_w_given_tag, count_tag)\n",
        "    return (count_w_given_tag, count_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHQUzvE-jsjD"
      },
      "source": [
        "# compute  Transition Probability\n",
        "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
        "    tags = [pair[1] for pair in train_bag]\n",
        "    count_t1 = len([t for t in tags if t==t1])\n",
        "    count_t2_t1 = 0\n",
        "    for index in range(len(tags)-1):\n",
        "        if tags[index]==t1 and tags[index+1] == t2:\n",
        "            count_t2_t1 += 1\n",
        "    return (count_t2_t1, count_t1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D86dMPprjsjF"
      },
      "source": [
        "# creating t x t transition matrix of tags, t= no of tags\n",
        "# Matrix(i, j) represents P(jth tag after the ith tag)\n",
        " \n",
        "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
        "for i, t1 in enumerate(list(tags)):\n",
        "    for j, t2 in enumerate(list(tags)): \n",
        "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
        "print(tags_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfMxa_LujsjH"
      },
      "source": [
        "# convert the matrix to a df for better readability\n",
        "# the table is same as the transition table shown in section 3 of article\n",
        "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
        "display(tags_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8WeTS4FjsjI"
      },
      "source": [
        "def Viterbi(words, train_bag = train_tagged_words):\n",
        "    state = []\n",
        "    T = list(set([pair[1] for pair in train_bag]))\n",
        "     \n",
        "    for key, word in enumerate(words):\n",
        "        #initialise list of probability column for a given observation\n",
        "        p = [] \n",
        "        for tag in T:\n",
        "            if key == 0:\n",
        "                transition_p = tags_df.loc['DELM', tag]\n",
        "            else:\n",
        "                transition_p = tags_df.loc[state[-1], tag]\n",
        "                 \n",
        "            # compute emission and state probabilities\n",
        "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
        "            state_probability = emission_p * transition_p    \n",
        "            p.append(state_probability)\n",
        "             \n",
        "        pmax = max(p)\n",
        "        # getting state for which probability is maximum\n",
        "        state_max = T[p.index(pmax)] \n",
        "        state.append(state_max)\n",
        "    return list(zip(words, state))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF7hvFAqjsjJ"
      },
      "source": [
        "# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n",
        "random.seed(1234)      #define a random seed to get same sentences when run multiple times\n",
        " \n",
        "# choose random 10 numbers\n",
        "rndom = [ random.randint(1, len(test_data)) for x in range(10) ]\n",
        " \n",
        "# list of 10 sents on which we test the model\n",
        "test_run = [test_data[i] for i in rndom]\n",
        " \n",
        "# list of tagged words\n",
        "test_run_base = [tup for sent in test_run for tup in sent]\n",
        " \n",
        "# list of untagged words\n",
        "test_tagged_words = [tup[0] for sent in test_run for tup in sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEA7x2v1jsjL"
      },
      "source": [
        "#Here We will only test 10 sentences to check the accuracy\n",
        "#as testing the whole training set takes huge amount of time\n",
        "start = time.time()\n",
        "tagged_seq = Viterbi(test_tagged_words)\n",
        "end = time.time()\n",
        "difference = end-start\n",
        " \n",
        "print(\"Time taken in seconds: \", difference)\n",
        " \n",
        "# accuracy\n",
        "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j]\n",
        " \n",
        "accuracy = len(check)/len(tagged_seq)\n",
        "print('Viterbi Algorithm Accuracy On Test: ',accuracy*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgsWr-YoDkbZ"
      },
      "source": [
        "# Let's test our Viterbi algorithm on a few sample sentences of train dataset\n",
        "random.seed(1234)      #define a random seed to get same sentences when run multiple times\n",
        " \n",
        "# choose random 10 numbers\n",
        "rndom = [random.randint(1,len(train_data)) for x in range(10)]\n",
        " \n",
        "# list of 10 sents on which we test the model\n",
        "train_run = [train_data[i] for i in rndom]\n",
        " \n",
        "# list of tagged words\n",
        "train_run_base = [tup for sent in train_run for tup in sent]\n",
        " \n",
        "# list of untagged words\n",
        "train_tagged_words = [tup[0] for sent in train_run for tup in sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNWYiyLQDkkx"
      },
      "source": [
        "#Here We will only train 10 sentences to check the accuracy\n",
        "#as testing the whole training set takes huge amount of time\n",
        "start = time.time()\n",
        "tagged_seq = Viterbi(train_tagged_words)\n",
        "end = time.time()\n",
        "difference = end-start\n",
        " \n",
        "print(\"Time taken in seconds: \", difference)\n",
        " \n",
        "# accuracy\n",
        "check = [i for i, j in zip(tagged_seq, train_run_base) if i == j]\n",
        " \n",
        "accuracy = len(check)/len(tagged_seq)\n",
        "print('Viterbi Algorithm Accuracy On Train: ',accuracy*100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}